{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b9f534",
   "metadata": {},
   "source": [
    "<input aria-invalid=\"false\" id=\"ifl-InputFormField-ihl-useId-passport-webapp-1\" name=\"__email\" aria-required=\"true\" type=\"email\" autocomplete=\"email\" autocorrect=\"off\" autocapitalize=\"none\" class=\"css-1tljng6 e1jgz0i3\" value=\"yue227@hotmail.com\">\n",
    "\n",
    "<input aria-invalid=\"false\" id=\"ifl-InputFormField-ihl-useId-passport-webapp-1\" name=\"__password\" aria-required=\"true\" autocomplete=\"current-password\" type=\"password\" class=\"css-1tljng6 e1jgz0i3\" value=\"!Q!@1q12\">\n",
    "\n",
    "\n",
    "<input type=\"text\" aria-invalid=\"false\" id=\"text-input-what\" name=\"q\" aria-haspopup=\"listbox\" enterkeyhint=\"search\" autocapitalize=\"none\" autocomplete=\"off\" aria-autocomplete=\"list\" aria-controls=\"text-input-what\" placeholder=\"Job title, keywords, or company\" value=\"\" aria-label=\"search: Job title, keywords, or company\" maxlength=\"512\" class=\"css-46u1mk e1jgz0i3\">\n",
    "\n",
    "\n",
    "<button class=\"yosegi-InlineWhatWhere-primaryButton\" type=\"submit\">Search</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2f51c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set the path to your ChromeDriver executable\n",
    "chromedriver_path = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver_path\n",
    "\n",
    "# Create a new instance of the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the login page\n",
    "driver.get(\"https://www.indeed.com/account/login\")\n",
    "\n",
    "# Find the email input field and enter your email address\n",
    "email_input = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.ID, \"ifl-InputFormField-ihl-useId-passport-webapp-1\"))\n",
    ")\n",
    "email_input.send_keys(\"yue227@hotmail.com\")\n",
    "\n",
    "# Find the \"Continue\" button and click it\n",
    "continue_button = WebDriverWait(driver, 20).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-tn-element='auth-page-email-submit-button']\"))\n",
    ")\n",
    "continue_button.click()\n",
    "\n",
    "\n",
    "continue_button = WebDriverWait(driver, 20).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-tn-element='auth-page-email-submit-button']\"))\n",
    ")\n",
    "continue_button.click()\n",
    "\n",
    "# Find the password input field and wait for it to be clickable\n",
    "password_input = WebDriverWait(driver, 20).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[type='password']\"))\n",
    ")\n",
    "\n",
    "# Clear any existing text in the password field\n",
    "password_input.clear()\n",
    "\n",
    "# Enter your password\n",
    "password_input.send_keys(\"!Q!@1q12\")\n",
    "\n",
    "# Submit the login form\n",
    "password_input.send_keys(Keys.ENTER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d677443",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.ID, \"text-input-what\"))\n",
    ")\n",
    "job_titles.send_keys(\"data scientist\")\n",
    "job_titles.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b3263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c14d7d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 143\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_url\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[43mget_jobs_of_title\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata+scientist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[88], line 49\u001b[0m, in \u001b[0;36mget_jobs_of_title\u001b[1;34m(job_title)\u001b[0m\n\u001b[0;32m     46\u001b[0m url \u001b[38;5;241m=\u001b[39m head\u001b[38;5;241m+\u001b[39mtail \n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#get links to webpages of jobs on the joblist\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m job_page_links \u001b[38;5;241m=\u001b[39m \u001b[43mget_job_links_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job_page_link \u001b[38;5;129;01min\u001b[39;00m job_page_links:\n\u001b[0;32m     52\u001b[0m     gap \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(job_gap_min,job_gap_max) \n",
      "Cell \u001b[1;32mIn[88], line 70\u001b[0m, in \u001b[0;36mget_job_links_from_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mThis function gets the links of the jobs on the joblist page.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    job_page_links (list): list of links to the webpages of the jobs\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m job_page_links \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 70\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mget_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, href\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/rc/clk?jk=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(item) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfccid=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(item):\n",
      "Cell \u001b[1;32mIn[88], line 16\u001b[0m, in \u001b[0;36mget_soup\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03mThis function get the beautifulsoup object of a webpage.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    soup (obj): beautifulsoup object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m request \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResistance is futile\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BeautifulSoup(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#job_titles = [\"data+scientist\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    \"\"\"\n",
    "    This function get the beautifulsoup object of a webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): the link string of webpage\n",
    "\n",
    "    Returns:\n",
    "        soup (obj): beautifulsoup object\n",
    "    \"\"\"\n",
    "    request = Request(url, headers={'User-Agent': 'Resistance is futile'})\n",
    "    response = urlopen(request)\n",
    "    return BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "def get_jobs_of_title(job_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        job_title (str): example: 'data+scientist'\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    #needed to be changed\n",
    "    num_pages = 1 #number of pages to scrape\n",
    "    page_gap_min = 3 #min sleep time between pages\n",
    "    page_gap_max = 5 #max sleep time between pages\n",
    "    job_per_page = 50 #number of jobs in one page\n",
    "    job_gap_min = 5 #min sleep time between jobs\n",
    "    job_gap_max = 6 #max sleep time between jobs\n",
    "\n",
    "    for i in range(num_pages): \n",
    "        #sleep between each call\n",
    "        gap = random.uniform(page_gap_min,page_gap_max) \n",
    "        time.sleep(gap)\n",
    "\n",
    "        #each page contains 50 jobs\n",
    "        tail = \"jobs?q={0}&sort=date&limit={1}\".format(job_title,job_per_page)\n",
    "        if i>0:\n",
    "            tail += \"&start={0}\".format(i*job_per_page)\n",
    "\n",
    "        #get link to joblist page\n",
    "        url = head+tail \n",
    "         \n",
    "        #get links to webpages of jobs on the joblist\n",
    "        job_page_links = get_job_links_from_page(url)\n",
    "\n",
    "        for job_page_link in job_page_links:\n",
    "            gap = random.uniform(job_gap_min,job_gap_max) \n",
    "            time.sleep(gap)\n",
    "            data = get_info_from_job_page(job_page_link)\n",
    "\n",
    "            print(json.dumps(data))\n",
    "\n",
    "def get_job_links_from_page(url):\n",
    "    \"\"\"\n",
    "    This function gets the links of the jobs on the joblist page.\n",
    "\n",
    "    Args:\n",
    "        url (str): link to joblist page\n",
    "\n",
    "    Returns:\n",
    "        job_page_links (list): list of links to the webpages of the jobs\n",
    "    \"\"\"\n",
    "\n",
    "    job_page_links = []\n",
    "    soup = get_soup(url)\n",
    "    for item in soup.find_all(\"a\", href=True):\n",
    "        if '/rc/clk?jk=' in str(item) and 'fccid=' in str(item):\n",
    "            link = item['href'].split(\"clk?\")[1]\n",
    "            job_page_links.append(head+'viewjob?'+link)\n",
    "    return job_page_links\n",
    "\n",
    "def get_info_from_job_page(url):\n",
    "    \"\"\"\n",
    "    This function get all the useful info from the job webpage.\n",
    "\n",
    "    Args:\n",
    "        url (str): link to job webpage\n",
    "\n",
    "    Returns:\n",
    "        data (dict): dictionary with keywords: \n",
    "                     time_stamp, original_link, job_title, location, company, description\n",
    "    \"\"\"\n",
    "    soup = get_soup(url)\n",
    "    data = {}\n",
    "    time_str = soup.find('div',class_='result-link-bar').find('span').getText()\n",
    "\n",
    "    try:\n",
    "        data[\"time_stamp\"] = get_timestamp(time_str).strftime(\"%d-%m-%Y %H:%M\")\n",
    "        data[\"job_title\"] = soup.find('b', class_='jobtitle').getText()\n",
    "        data[\"location\"] = soup.find('span', class_='location').getText()\n",
    "        data[\"company\"] = soup.find('span', class_='company').getText()\n",
    "        data[\"description\"] = soup.find('td',class_='snip').find('div').getText()\n",
    "\n",
    "        re_link = soup.find('a',class_='sl ws_label')['href'].split(\"&from=\")[0]\n",
    "        re_link = head[:-1]+re_link\n",
    "        data[\"original_link\"] = get_original_link(re_link)\n",
    "    except:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def get_timestamp(time_str):\n",
    "    \"\"\"\n",
    "    Calculate the timestamp from the time string.\n",
    "    \n",
    "    Args:\n",
    "        time_str (str): time string, like '2 hours ago'\n",
    "\n",
    "    Returns:\n",
    "        time_stamp (obj): timestamp object\n",
    "    \"\"\"\n",
    "    if 'hour' in time_str:\n",
    "        lag = int(time_str.split('hour')[0])\n",
    "        delta = timedelta(hours=lag)\n",
    "        now = datetime.utcnow().replace(second=0,minute=0)\n",
    "        return now-delta\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_original_link(url):\n",
    "    \"\"\"\n",
    "    Get the original link of the job description.\n",
    "    \n",
    "    Args:\n",
    "        url (str): the link in Indeed database\n",
    "\n",
    "    Returns:\n",
    "        url (str): the original link to the job description\n",
    "    \"\"\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    original_url = driver.current_url\n",
    "    driver.quit()\n",
    "    return original_url\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_jobs_of_title(\"data+scientist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a297a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f3d98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scrapset import indeed\n",
    "scraper = indeed()\n",
    "data = scraper.indeed_jobs('https://ie.indeed.com', 40, 'data scientist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a849e843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': ['Data Scientist',\n",
       "  'Data Scientist (IC3)',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist - Legal & Compliance',\n",
       "  'Principal Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Digital Solution Area Specialists - Data & AI - Balkan region',\n",
       "  'Data Scientist Apprenticeship Programme',\n",
       "  'Data Scientist',\n",
       "  'Senior Data Scientist',\n",
       "  'Machine Learning Engineer',\n",
       "  'Machine Learning Internship',\n",
       "  'Senior Data Scientist',\n",
       "  'Machine Learning Scientist Intern',\n",
       "  'Data Analytics Developer',\n",
       "  'Data Scientist',\n",
       "  'Data Scientist',\n",
       "  'Senior Data Scientist',\n",
       "  'Senior Machine Learning Engineer',\n",
       "  'Senior Data Scientist',\n",
       "  'Sr. Principal Scientist - Value, Evidence and Real World Evidence Data',\n",
       "  'Data Analytics and AI - Senior Manager - Data Governance',\n",
       "  'Senior Data Scientist',\n",
       "  'Senior Data Scientist',\n",
       "  'Deep Learning Engineer',\n",
       "  'Regional Vice President, Data Science',\n",
       "  'R&D Deep Learning Engineer',\n",
       "  'Data Analytics and AI - Senior Manager - Commercial Analytics',\n",
       "  'Senior Data Scientist',\n",
       "  'Machine Learning Software Intern',\n",
       "  'Data Science (DS) Engineer',\n",
       "  'Senior Clinical Data Scientist (Trial Data Manager)',\n",
       "  'Statistician - Real World Evidence',\n",
       "  'Senior Manager, Artificial Intelligence & Machine Learning',\n",
       "  '2/2 Principal Clinical Data Scientist (Trial Data Manager/Project Data',\n",
       "  'Head of Analytics',\n",
       "  'Data & AI Lead',\n",
       "  'Inventory and Supply Chain Analytics Manager',\n",
       "  'Director Data Science & AI',\n",
       "  'Data Scientist, Product Analytics',\n",
       "  'Sr ML Engineer - Research & Development',\n",
       "  'Analyst/Senior Analyst - AI/ML Engineer',\n",
       "  'Graduate Programme - Deep Learning Research Engineer - 2 Year Fixed Term Contract',\n",
       "  'Machine Learning Ops Engineer',\n",
       "  'AI Engineer Academy - Ireland',\n",
       "  'Sr Principal Clinical Data Scientist (Project Data Manager)',\n",
       "  'Machine Learning Researcher Intern',\n",
       "  'Sr Data Informatics Analyst - Cloud Analytics',\n",
       "  'Digital Technical Specialist, Azure Data & AI - German Speaker',\n",
       "  'Senior Data Scientist / Scientific Data Programmer Analyst',\n",
       "  'Head of Data Science',\n",
       "  'Deep Learning Manager',\n",
       "  'Principal Machine Learning Engineer',\n",
       "  'Senior Machine Learning Engineer (Lead Optimization) (Remote - Ireland)',\n",
       "  'Data Scientist – Cork, Ireland',\n",
       "  '2024 Graduate Opportunities – Data Analytics & AI',\n",
       "  \"Data Scientist (NLP, Custom LLM's, Ai Agents)\",\n",
       "  'Machine Learning Engineer',\n",
       "  'Pharmaceutical Data Scientist'],\n",
       " 'all_details': ['IBM\\nDublin, County Dublin',\n",
       "  'Microsoft\\nDublin, County Dublin',\n",
       "  'Konnekt-able Technologies\\nWaterford, County Waterford',\n",
       "  'ESB\\nHybrid remote in Dublin, County Dublin',\n",
       "  'Orcawise\\nRemote',\n",
       "  'Optum\\nDublin, County Dublin',\n",
       "  'The Ardonagh Group\\nMullingar, County Westmeath',\n",
       "  'Microsoft\\nDublin, County Dublin',\n",
       "  'Idiro Analytics\\nDublin, County Dublin',\n",
       "  'Bentley Systems\\nSouth Dublin, County Dublin',\n",
       "  'Flutter Entertainment\\nDublin, County Dublin',\n",
       "  'Reperio Human Capital\\nCork, County Cork',\n",
       "  'Cadence Design Systems\\nCork, County Cork',\n",
       "  'Stantec\\nDublin, County Dublin',\n",
       "  'Integral Ad Science\\nDublin, County Dublin',\n",
       "  'Ornua Co-operative Limited\\nHybrid remote in Dublin, County Dublin',\n",
       "  'Morgan McKinley\\nDublin, County Dublin',\n",
       "  'Reperio Human Capital\\nDublin, County Dublin',\n",
       "  'Outmin Ltd\\nDublin, County Dublin',\n",
       "  'Mavenoid\\nRemote',\n",
       "  'McKesson\\nCork, County Cork',\n",
       "  'Lilly\\nHybrid remote in Cork, County Cork',\n",
       "  'EY\\nHybrid remote in South Dublin, County Dublin',\n",
       "  'Vectra\\nDublin, County Dublin',\n",
       "  'Eolas Recruitment\\nDublin, County Dublin',\n",
       "  'Valeo\\nTuam, County Galway',\n",
       "  'Integral Ad Science\\nDublin, County Dublin',\n",
       "  'Valeo\\nTuam, County Galway',\n",
       "  'EY\\nHybrid remote in South Dublin, County Dublin',\n",
       "  'Reperio Human Capital\\nDublin, County Dublin',\n",
       "  'Arm\\nHybrid remote in Galway, County Galway',\n",
       "  'INTEL\\nLeixlip, County Kildare',\n",
       "  'Novartis\\nDublin, County Dublin',\n",
       "  'Stryker\\nCork, County Cork',\n",
       "  'DocuSign\\nDublin, County Dublin',\n",
       "  'Novartis\\nDublin, County Dublin',\n",
       "  'Mediolanum International Funds Limited\\nDublin, County Dublin',\n",
       "  'Accenture\\nDublin, County Dublin',\n",
       "  'Zoetis\\nDublin, County Dublin',\n",
       "  'Novartis\\nDublin, County Dublin',\n",
       "  'Meta\\nDublin, County Dublin',\n",
       "  'Xperi\\nGalway, County Galway',\n",
       "  'Lilly\\nHybrid remote in Cork, County Cork',\n",
       "  'Valeo\\nTuam, County Galway',\n",
       "  'Reperio Human Capital\\nDublin, County Dublin',\n",
       "  'Avanade\\nDublin, County Dublin',\n",
       "  'Novartis\\nDublin, County Dublin',\n",
       "  'Integral Ad Science\\nDublin, County Dublin',\n",
       "  'ServiceNow\\nDublin, County Dublin',\n",
       "  'Microsoft\\nDublin, County Dublin',\n",
       "  'Irish Recruitment\\nHybrid remote in Dublin, County Dublin',\n",
       "  'Applegreen Stores\\nPark West, County Dublin',\n",
       "  'Valeo\\nTuam, County Galway',\n",
       "  'Mastercard\\nDublin, County Dublin',\n",
       "  'Yelp\\nGalway, County Galway',\n",
       "  'AtData\\nCork, County Cork',\n",
       "  'BearingPoint Ireland\\nHybrid remote in Dublin, County Dublin',\n",
       "  'Orcawise\\nRemote',\n",
       "  'PIB Group\\nRemote',\n",
       "  'Westbourne IT Global Services\\nHybrid remote in Cork, County Cork'],\n",
       " 'salary': []}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b891eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6d8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
